BARD (Bidirectional Attention-based Representations for Documents) is an advanced model used for text summarization, focusing primarily on extractive summarization tasks. It is similar to BERT but optimized specifically for document-level understanding. Here's how BARD works for text summarization:

### Overview
- **Type**: Encoder-Only Model
- **Architecture**: Uses a transformer-based architecture with multiple self-attention layers, similar to BERT, but optimized for understanding long documents.
- **Purpose**: Extracts important sentences from a document to form concise summaries by leveraging bidirectional attention mechanisms.

### Key Features
- **Bidirectional Attention**: Processes the input text in both directions (forward and backward) to understand the full context, making it effective for identifying key sentences.
- **Extractive Approach**: Selects the most relevant sentences or passages directly from the input text, ensuring the summary remains accurate and close to the original content.



{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc607a5d-1314-47c7-a762-e40a877d010f",
   "metadata": {},
   "source": [
    "When building a text summarizer or any other text-based model, text preprocessing, class imbalance checking, and exploratory data analysis (EDA) are crucial for several reasons. Even though your goal is to generate summaries, these steps help ensure that your model performs effectively and reliably. Here's is the reason why............\n",
    "---\n",
    "\n",
    "### 1. **Text Preprocessing:**\n",
    "   Text data is often noisy and unstructured, so preprocessing helps to clean and standardize it, which is essential for machine learning models. In a text summarization task, preprocessing typically includes:\n",
    "   - **Tokenization**: Splitting text into smaller units (tokens) so the model can process it.\n",
    "   - **Stopwords removal**: Words like \"is\", \"the\", etc., do not carry much meaning for summarization and can be removed to focus on more important words.\n",
    "   - **Lowercasing**: Ensures consistency, as \"Text\" and \"text\" should be treated as the same word.\n",
    "   - **Lemmatization/Stemming**: Converts words to their base forms to avoid treating different forms of the same word as distinct, improving model accuracy.\n",
    "   - **Handling punctuation and special characters**: These may not be needed for summarization unless specifically required (e.g., if they influence the meaning).\n",
    "\n",
    "---\n",
    "### 2. **Class Imbalance in Text Modeling:**\n",
    "   Even in text summarization, imbalance can occur. For example, if certain types of documents (long scientific papers vs. short news articles) dominate your dataset, your model might perform poorly on underrepresented types. In tasks like text classification or summarization, class imbalance can cause the model to:\n",
    "   - **Overfit on the majority class**: The model might generate better summaries for more frequent types of text while performing poorly on others.\n",
    "   - **Misrepresent data distribution**: If your dataset has an imbalance between abstract lengths or text types, the model might learn to favor one pattern over others, leading to biased summaries.\n",
    "\n",
    "  ---\n",
    "### 3. **Exploratory Data Analysis (EDA):**\n",
    "   EDA is crucial even for text summarization because it helps you understand the nature of the data you're working with. It can provide insights into:\n",
    "   - **Word/character distributions**: Understanding common terms and their frequencies might reveal patterns that could impact the summarization process.\n",
    "   - **Sentence length distributions**: Shorter texts might require different summarization strategies than longer ones, and EDA helps identify this.\n",
    "   - **Outliers**: You may have outliers (e.g., extremely long articles) that could skew your modelâ€™s performance, and identifying these helps ensure fair evaluation.\n",
    "   - **Patterns in structure**: Some texts may have specific formats (e.g., journal abstracts vs. opinion pieces), and EDA helps uncover such structural elements that could influence the summarization.\n",
    "\n",
    "  ---\n",
    "### 4. **Other Tasks like Feature Extraction:**\n",
    "   - **Sentence Embeddings**: For summarization models, embedding text into meaningful vectors allows the model to understand semantic content. Without proper feature extraction, the summarizer won't understand the context well.\n",
    "   - **Stopword and Keyword Identification**: Identifying keywords or key phrases can help build better summaries by focusing on core content rather than filler words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce97acb0-c644-46c6-85d5-5a8de8ce1815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20df913-85b9-4f39-8252-906896a5ac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133215, 2)\n"
     ]
    }
   ],
   "source": [
    "dataset_shape = df.shape\n",
    "print(dataset_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24487221-a938-47b4-a39e-b8b5dcb6aa66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article     2692\n",
      "abstract       0\n",
      "dtype: int64\n",
      "total dublicates = 81\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "duplicates = df.duplicated().sum()\n",
    "print (missing_values)\n",
    "print(\"total dublicates =\",duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29493d86-b4cc-4320-bb75-b5e5c878b134",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article     0\n",
      "abstract    0\n",
      "dtype: int64\n",
      "total dublicates = 0\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = df.dropna()\n",
    "df_cleaned = df_cleaned.drop_duplicates()\n",
    "missing_values = df_cleaned.isnull().sum()\n",
    "duplicates = df_cleaned.duplicated().sum()\n",
    "print (missing_values)\n",
    "print(\"total dublicates =\",duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5009742d-6db3-4f27-bc4f-bd24bcf6c699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified sample saved to: stratified_sample.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Use the 'article' column for stratification based on length\n",
    "column_name = 'article'  # Updated column name\n",
    "\n",
    "if column_name in df_cleaned.columns:\n",
    "    # Add a column for article length\n",
    "    df_cleaned['paragraph_length'] = df_cleaned[column_name].apply(len)\n",
    "else:\n",
    "    print(f\"Column '{column_name}' not found. Please verify the column name.\")\n",
    "\n",
    "# If the column exists and is correctly processed, continue with the stratified sampling\n",
    "if 'paragraph_length' in df_cleaned.columns:\n",
    "    # Define bins for stratified sampling based on paragraph length\n",
    "    bins = [0, 50, 100, 200, 300, 500, 1000, np.inf]\n",
    "    labels = ['0-50', '51-100', '101-200', '201-300', '301-500', '501-1000', '1000+']\n",
    "    df_cleaned['length_bin'] = pd.cut(df_cleaned['paragraph_length'], bins=bins, labels=labels)\n",
    "\n",
    "    # Calculate the proportionate number of samples per bin to total 5000 rows\n",
    "    total_rows = 5000\n",
    "    bin_counts = df_cleaned['length_bin'].value_counts()\n",
    "    bin_proportions = bin_counts / bin_counts.sum()\n",
    "    bin_sample_sizes = (bin_proportions * total_rows).round().astype(int)\n",
    "\n",
    "    # Stratified sampling based on calculated sample sizes\n",
    "    sampled_df_cleaned = pd.concat([\n",
    "        df_cleaned[df_cleaned['length_bin'] == bin].sample(n=min(bin_sample_sizes[bin], len(df_cleaned[df_cleaned['length_bin'] == bin])), random_state=42)\n",
    "        for bin in bin_sample_sizes.index\n",
    "    ])\n",
    "\n",
    "    # Adjust the sample size to exactly 5000 rows if needed\n",
    "    if len(sampled_df_cleaned) > 5000:\n",
    "        sampled_df_cleaned = sampled_df_cleaned.sample(n=5000, random_state=42)\n",
    "    elif len(sampled_df_cleaned) < 5000:\n",
    "        additional_samples_needed = 5000 - len(sampled_df_cleaned)\n",
    "        additional_samples = df_cleaned.sample(n=additional_samples_needed, random_state=42)\n",
    "        sampled_df_cleaned = pd.concat([sampled_df_cleaned, additional_samples])\n",
    "\n",
    "    # Drop the helper columns used for stratification\n",
    "    sampled_df_cleaned = sampled_df_cleaned.drop(columns=['paragraph_length', 'length_bin'])\n",
    "\n",
    "    # Save the sampled data to a new CSV file\n",
    "    output_file_path = 'stratified_sample.csv'\n",
    "    sampled_df_cleaned.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Stratified sample saved to: {output_file_path}\")\n",
    "else:\n",
    "    print(\"Paragraph length column could not be created. Please check the column name.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34d765ba-0f4a-4b09-802c-76d466a18b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a25a9-ee42-430c-b064-41a63c20d050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
